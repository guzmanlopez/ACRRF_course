
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{trabajo-final}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Practice sessions - Course 1 and Course
3}\label{practice-sessions---course-1-and-course-3}

\subsection{CART Trees and Random Forests - Jean‐Michel
POGGI}\label{cart-trees-and-random-forests---jeanmichel-poggi}

\subsubsection{Master 2 Course in
Statistics}\label{master-2-course-in-statistics}

\subsubsection{Universidad de la República -- Facultad de Ingeniería,
Montevideo,
Uruguay}\label{universidad-de-la-repuxfablica-facultad-de-ingenieruxeda-montevideo-uruguay}

\subsubsection{February 2018}\label{february-2018}

\paragraph{\texorpdfstring{Guide for the practice sessions with the
companion scenario, the documentation
\url{cran.r-project.org/web/packages/VSURF/index.html} and the two
articles:
\url{journal.r-project.org/archive/2015-2/genuer-poggi-tuleaumalot.pdf}
\url{hal-descartes.archives-ouvertes.fr/hal-01387654v2}}{Guide for the practice sessions with the companion scenario, the documentation cran.r-project.org/web/packages/VSURF/index.html and the two articles: journal.r-project.org/archive/2015-2/genuer-poggi-tuleaumalot.pdf hal-descartes.archives-ouvertes.fr/hal-01387654v2}}\label{guide-for-the-practice-sessions-with-the-companion-scenario-the-documentation-cran.r-project.orgwebpackagesvsurfindex.html-and-the-two-articles-journal.r-project.orgarchive2015-2genuer-poggi-tuleaumalot.pdf-hal-descartes.archives-ouvertes.frhal-01387654v2}

    \subsection{1. Data}\label{data}

    \paragraph{\texorpdfstring{1. Load the library
\textbf{kernlab}}{1. Load the library kernlab}}\label{load-the-library-kernlab}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Load library}
        \PY{k+kn}{library}\PY{p}{(}kernlab\PY{p}{)}
\end{Verbatim}


    \paragraph{\texorpdfstring{2. Load the dataset \textbf{spam} in R and
build the dataframes of learning and test sets (the first will be used
for designing trees, the second for evaluating
errors)}{2. Load the dataset spam in R and build the dataframes of learning and test sets (the first will be used for designing trees, the second for evaluating errors)}}\label{load-the-dataset-spam-in-r-and-build-the-dataframes-of-learning-and-test-sets-the-first-will-be-used-for-designing-trees-the-second-for-evaluating-errors}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Load data}
        data\PY{p}{(}spam\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Explore dataset}
        \PY{c+c1}{\PYZsh{} ?spam}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} str(spam)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kp}{head}\PY{p}{(}spam\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|llllllllllllllllllllllllllllllllllllllllllllllllllllllllll}
 make & address & all & num3d & our & over & remove & internet & order & mail & ⋯ & charSemicolon & charRoundbracket & charSquarebracket & charExclamation & charDollar & charHash & capitalAve & capitalLong & capitalTotal & type\\
\hline
	 0.00  & 0.64  & 0.64  & 0     & 0.32  & 0.00  & 0.00  & 0.00  & 0.00  & 0.00  & ⋯     & 0.00  & 0.000 & 0     & 0.778 & 0.000 & 0.000 & 3.756 &  61   &  278  & spam \\
	 0.21  & 0.28  & 0.50  & 0     & 0.14  & 0.28  & 0.21  & 0.07  & 0.00  & 0.94  & ⋯     & 0.00  & 0.132 & 0     & 0.372 & 0.180 & 0.048 & 5.114 & 101   & 1028  & spam \\
	 0.06  & 0.00  & 0.71  & 0     & 1.23  & 0.19  & 0.19  & 0.12  & 0.64  & 0.25  & ⋯     & 0.01  & 0.143 & 0     & 0.276 & 0.184 & 0.010 & 9.821 & 485   & 2259  & spam \\
	 0.00  & 0.00  & 0.00  & 0     & 0.63  & 0.00  & 0.31  & 0.63  & 0.31  & 0.63  & ⋯     & 0.00  & 0.137 & 0     & 0.137 & 0.000 & 0.000 & 3.537 &  40   &  191  & spam \\
	 0.00  & 0.00  & 0.00  & 0     & 0.63  & 0.00  & 0.31  & 0.63  & 0.31  & 0.63  & ⋯     & 0.00  & 0.135 & 0     & 0.135 & 0.000 & 0.000 & 3.537 &  40   &  191  & spam \\
	 0.00  & 0.00  & 0.00  & 0     & 1.85  & 0.00  & 0.00  & 1.85  & 0.00  & 0.00  & ⋯     & 0.00  & 0.223 & 0     & 0.000 & 0.000 & 0.000 & 3.000 &  15   &   54  & spam \\
\end{tabular}


    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Build dataframes with stratified sampling}
        \PY{k+kn}{library}\PY{p}{(}dplyr\PY{p}{)}
        
        \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{1}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Add id column to spam data.frame}
        spam\PY{o}{\PYZdl{}}ID \PY{o}{\PYZlt{}\PYZhy{}} \PY{l+m}{1}\PY{o}{:}\PY{k+kp}{nrow}\PY{p}{(}spam\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Learn data.frame}
        learn \PY{o}{\PYZlt{}\PYZhy{}} 
          spam \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
          group\PYZus{}by\PY{p}{(}type\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
          sample\PYZus{}frac\PY{p}{(}\PY{l+m}{0.70}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Test data.frame}
        test \PY{o}{\PYZlt{}\PYZhy{}} spam\PY{p}{[}\PY{o}{\PYZhy{}}learn\PY{o}{\PYZdl{}}ID\PY{p}{,}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Remove ID column}
        learn \PY{o}{\PYZlt{}\PYZhy{}} learn\PY{p}{[}\PY{p}{,}\PY{l+m}{\PYZhy{}59}\PY{p}{]}
        test \PY{o}{\PYZlt{}\PYZhy{}} test\PY{p}{[}\PY{p}{,}\PY{l+m}{\PYZhy{}59}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


    \end{Verbatim}

    \subsection{2. CART trees}\label{cart-trees}

    \subsubsection{\texorpdfstring{1. Load the library
\textbf{rpart}}{1. Load the library rpart}}\label{load-the-library-rpart}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{library}\PY{p}{(}rpart\PY{p}{)}
\end{Verbatim}


    \subsubsection{\texorpdfstring{2. Compute the default tree provided by
\textbf{rpart}}{2. Compute the default tree provided by rpart}}\label{compute-the-default-tree-provided-by-rpart}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Default rpart tree with learn data}
        fit.learn.def \PY{o}{\PYZlt{}\PYZhy{}} rpart\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} learn\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} Depth of the default rpart tree}
        depth.def \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{max}\PY{p}{(}rpart\PY{o}{:::}tree.depth\PY{p}{(}\PY{k+kp}{as.numeric}\PY{p}{(}\PY{k+kp}{rownames}\PY{p}{(}fit.learn.def\PY{o}{\PYZdl{}}frame\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{k+kp}{cat}\PY{p}{(}\PY{k+kp}{paste0}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{depth of default rpart tree: \PYZdq{}}\PY{p}{,} depth.def\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
depth of default rpart tree: 5
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} plot\PY{p}{(}fit.learn.def\PY{p}{,} uniform \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Default rpart tree\PYZdq{}}\PY{p}{)}
        text\PY{p}{(}fit.learn.def\PY{p}{,} cex \PY{o}{=} \PY{l+m}{0.8}\PY{p}{,} use.n \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} xpd \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{darkred\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{3. Build a tree of depth 1 (stump) and draw
it}\label{build-a-tree-of-depth-1-stump-and-draw-it}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} fit.learn.d1 \PY{o}{\PYZlt{}\PYZhy{}} rpart\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} learn\PY{p}{,} maxdepth \PY{o}{=} \PY{l+m}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Check the epth of the 1d tree}
         depth.d1 \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{max}\PY{p}{(}rpart\PY{o}{:::}tree.depth\PY{p}{(}\PY{k+kp}{as.numeric}\PY{p}{(}\PY{k+kp}{rownames}\PY{p}{(}fit.learn.d1\PY{o}{\PYZdl{}}frame\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{k+kp}{cat}\PY{p}{(}\PY{k+kp}{paste0}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{depth of tree: \PYZdq{}}\PY{p}{,} depth.d1\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
depth of tree: 1
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} plot\PY{p}{(}fit.learn.d1\PY{p}{,} uniform \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Tree of depth = 1\PYZdq{}}\PY{p}{)}
         text\PY{p}{(}fit.learn.d1\PY{p}{,} cex \PY{o}{=} \PY{l+m}{0.8}\PY{p}{,} use.n \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} xpd \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{darkred\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{4. Examine splits primary splits and surrogate
splits}\label{examine-splits-primary-splits-and-surrogate-splits}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kp}{summary}\PY{p}{(}fit.learn.d1\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Call:
rpart(formula = type \textasciitilde{} ., data = learn, maxdepth = 1)
  n= 3221 

         CP nsplit rel error    xerror       xstd
1 0.4806935      0 1.0000000 1.0000000 0.02185313
2 0.0100000      1 0.5193065 0.5555556 0.01849238

Variable importance
 charDollar      num000       money capitalLong      credit      remove 
         48          16          14           7           7           7 

Node number 1: 3221 observations,    complexity param=0.4806935
  predicted class=nonspam  expected loss=0.393977  P(node) =1
    class counts:  1952  1269
   probabilities: 0.606 0.394 
  left son=2 (2417 obs) right son=3 (804 obs)
  Primary splits:
      charDollar      < 0.0555 to the left,  improve=504.8439, (0 missing)
      charExclamation < 0.0795 to the left,  improve=502.7630, (0 missing)
      remove          < 0.01   to the left,  improve=427.4809, (0 missing)
      free            < 0.075  to the left,  improve=386.5452, (0 missing)
      your            < 0.605  to the left,  improve=377.3568, (0 missing)
  Surrogate splits:
      num000      < 0.075  to the left,  agree=0.837, adj=0.346, (0 split)
      money       < 0.045  to the left,  agree=0.826, adj=0.301, (0 split)
      capitalLong < 71.5   to the left,  agree=0.790, adj=0.157, (0 split)
      credit      < 0.02   to the left,  agree=0.788, adj=0.150, (0 split)
      remove      < 0.01   to the left,  agree=0.787, adj=0.147, (0 split)

Node number 2: 2417 observations
  predicted class=nonspam  expected loss=0.2325197  P(node) =0.7503881
    class counts:  1855   562
   probabilities: 0.767 0.233 

Node number 3: 804 observations
  predicted class=spam     expected loss=0.1206468  P(node) =0.2496119
    class counts:    97   707
   probabilities: 0.121 0.879 


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} fit.learn.d1\PY{o}{\PYZdl{}}splits
\end{Verbatim}


    \begin{tabular}{r|lllll}
  & count & ncat & improve & index & adj\\
\hline
	charDollar & 3221        & -1          & 504.8438706 &  0.0555     & 0.0000000  \\
	charExclamation & 3221        & -1          & 502.7630189 &  0.0795     & 0.0000000  \\
	remove & 3221        & -1          & 427.4808712 &  0.0100     & 0.0000000  \\
	free & 3221        & -1          & 386.5452222 &  0.0750     & 0.0000000  \\
	your & 3221        & -1          & 377.3568215 &  0.6050     & 0.0000000  \\
	num000 &    0        & -1          &   0.8366967 &  0.0750     & 0.3457711  \\
	money &    0        & -1          &   0.8255200 &  0.0450     & 0.3009950  \\
	capitalLong &    0        & -1          &   0.7895064 & 71.5000     & 0.1567164  \\
	credit &    0        & -1          &   0.7879541 &  0.0200     & 0.1504975  \\
	remove &    0        & -1          &   0.7870227 &  0.0100     & 0.1467662  \\
\end{tabular}


    
    \subsubsection{5. Build a maximal tree and draw
it}\label{build-a-maximal-tree-and-draw-it}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} fit.learn.max \PY{o}{\PYZlt{}\PYZhy{}} rpart\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} learn\PY{p}{,} cp \PY{o}{=} \PY{l+m}{0}\PY{p}{,} minsplit \PY{o}{=} \PY{l+m}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Depth of the maximal tree}
         depth.max \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{max}\PY{p}{(}rpart\PY{o}{:::}tree.depth\PY{p}{(}\PY{k+kp}{as.numeric}\PY{p}{(}\PY{k+kp}{rownames}\PY{p}{(}fit.learn.max\PY{o}{\PYZdl{}}frame\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{k+kp}{cat}\PY{p}{(}\PY{k+kp}{paste0}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{depth of maximal tree: \PYZdq{}}\PY{p}{,} depth.max\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
depth of maximal tree: 30
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} plot\PY{p}{(}fit.learn.max\PY{p}{,} uniform \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Maximal tree\PYZdq{}}\PY{p}{)}
         text\PY{p}{(}fit.learn.max\PY{p}{,} cex \PY{o}{=} \PY{l+m}{0.5}\PY{p}{,} use.n \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} xpd \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{darkred\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_24_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{6. Draw the OOB errors of the Breiman's sequence of the
pruned subtrees of the maximal tree and interpret
it}\label{draw-the-oob-errors-of-the-breimans-sequence-of-the-pruned-subtrees-of-the-maximal-tree-and-interpret-it}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} printcp\PY{p}{(}fit.learn.max\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Classification tree:
rpart(formula = type \textasciitilde{} ., data = learn, cp = 0, minsplit = 0)

Variables actually used in tree construction:
 [1] address          all              business         capitalAve      
 [5] capitalLong      capitalTotal     charDollar       charExclamation 
 [9] charHash         charRoundbracket charSemicolon    conference      
[13] credit           cs               data             edu             
[17] email            font             free             george          
[21] hp               hpl              internet         mail            
[25] make             meeting          money            num000          
[29] num1999          num3d            num650           order           
[33] original         our              over             people          
[37] pm               project          re               remove          
[41] report           technology       will             you             
[45] your            

Root node error: 1269/3221 = 0.39398

n= 3221 

           CP nsplit rel error  xerror     xstd
1  0.48069346      0 1.0000000 1.00000 0.021853
2  0.14499606      1 0.5193065 0.55871 0.018530
3  0.04176517      2 0.3743105 0.47045 0.017378
4  0.02994484      4 0.2907801 0.34437 0.015315
5  0.01103231      5 0.2608353 0.30969 0.014638
6  0.01024429      6 0.2498030 0.27187 0.013831
7  0.00709220      7 0.2395587 0.26399 0.013653
8  0.00472813     14 0.1899133 0.24980 0.013322
9  0.00394011     17 0.1757289 0.22616 0.012741
10 0.00354610     18 0.1717888 0.22459 0.012701
11 0.00275808     22 0.1576044 0.22222 0.012641
12 0.00236407     24 0.1520883 0.21671 0.012498
13 0.00157604     28 0.1426320 0.21198 0.012373
14 0.00137904     49 0.1079590 0.21277 0.012394
15 0.00118203     53 0.1024429 0.21355 0.012415
16 0.00113825     55 0.1000788 0.21513 0.012456
17 0.00078802     69 0.0819543 0.21119 0.012352
18 0.00059102    119 0.0409771 0.20331 0.012140
19 0.00052535    123 0.0386131 0.20883 0.012289
20 0.00047281    129 0.0354610 0.21040 0.012331
21 0.00045030    134 0.0330969 0.21198 0.012373
22 0.00039401    149 0.0260047 0.23089 0.012861
23 0.00026267    198 0.0063042 0.23247 0.012900
24 0.00000000    213 0.0023641 0.23325 0.012920

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} plotcp\PY{p}{(}fit.learn.max\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{red\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{7. Find the best of them in the sense of an estimate
given by the cross-validation prediction
error}\label{find-the-best-of-them-in-the-sense-of-an-estimate-given-by-the-cross-validation-prediction-error}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} fit.learn.best.cv.cp \PY{o}{\PYZlt{}\PYZhy{}} fit.learn.max\PY{o}{\PYZdl{}}cptable\PY{p}{[}\PY{k+kp}{which.min}\PY{p}{(}fit.learn.max\PY{o}{\PYZdl{}}cptable\PY{p}{[}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{xerror\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{CP\PYZdq{}}\PY{p}{]}
         fit.learn.pruned.cv \PY{o}{\PYZlt{}\PYZhy{}} prune\PY{p}{(}fit.learn.max\PY{p}{,} cp \PY{o}{=} fit.learn.best.cv.cp\PY{p}{)}
\end{Verbatim}


    \subsubsection{8. Compare the default tree of rpart with the one
obtained by minimizing the prediction error. Same question with the one
obtained by applying the 1 SE
rule}\label{compare-the-default-tree-of-rpart-with-the-one-obtained-by-minimizing-the-prediction-error.-same-question-with-the-one-obtained-by-applying-the-1-se-rule}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} xerror \PYZlt{} min(xerror) + xstd}
         level.1se \PY{o}{\PYZlt{}\PYZhy{}} \PY{p}{(}\PY{k+kp}{which}\PY{p}{(}fit.learn.max\PY{o}{\PYZdl{}}cptable\PY{p}{[}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{xerror\PYZdq{}}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{k+kp}{min}\PY{p}{(}fit.learn.max\PY{o}{\PYZdl{}}cptable\PY{p}{[}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{xerror\PYZdq{}}\PY{p}{]}\PY{p}{)} \PY{o}{+} fit.learn.max\PY{o}{\PYZdl{}}cptable\PY{p}{[}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{xstd\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m}{1}\PY{p}{]}
         
         fit.learn.best.1se.cp \PY{o}{\PYZlt{}\PYZhy{}} fit.learn.max\PY{o}{\PYZdl{}}cptable\PY{p}{[}level.1se\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{CP\PYZdq{}}\PY{p}{]}
         fit.learn.pruned.1se \PY{o}{\PYZlt{}\PYZhy{}} prune\PY{p}{(}fit.learn.max\PY{p}{,} cp \PY{o}{=} fit.learn.best.1se.cp\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} par\PY{p}{(}mfrow \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,} \PY{l+m}{3}\PY{p}{)}\PY{p}{)}
         
         plot\PY{p}{(}fit.learn.def\PY{p}{,} uniform \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{rpart default\PYZdq{}}\PY{p}{)}
         text\PY{p}{(}fit.learn.def\PY{p}{,} cex \PY{o}{=} \PY{l+m}{0.6}\PY{p}{,} use.n \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} xpd \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{darkred\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}fit.learn.pruned.cv\PY{p}{,} uniform \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Minimizing pred. error\PYZdq{}}\PY{p}{)}
         text\PY{p}{(}fit.learn.pruned.cv\PY{p}{,} cex \PY{o}{=} \PY{l+m}{0.6}\PY{p}{,} use.n \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} xpd \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{darkgreen\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}fit.learn.pruned.1se\PY{p}{,} uniform \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{1 SE\PYZdq{}}\PY{p}{)}
         text\PY{p}{(}fit.learn.pruned.1se\PY{p}{,} cex \PY{o}{=} \PY{l+m}{0.6}\PY{p}{,} use.n \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} xpd \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{darkblue\PYZdq{}}\PY{p}{)}
         
         par\PY{p}{(}mfrow \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,} \PY{l+m}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{9. Compare the errors of the different trees obtained,
both in learning and in
test}\label{compare-the-errors-of-the-different-trees-obtained-both-in-learning-and-in-test}

    \paragraph{\texorpdfstring{Missclasification error of default
\textbf{rpart}
tree}{Missclasification error of default rpart tree}}\label{missclasification-error-of-default-rpart-tree}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} def.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}fit.learn.def\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}def.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} def.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|l}
 missc\_error\\
\hline
	 0.09637681\\
\end{tabular}


    
    \paragraph{Missclasification error of tree depth =
1}\label{missclasification-error-of-tree-depth-1}

    \paragraph{Maximal tree from test
data}\label{maximal-tree-from-test-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} d1.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}fit.learn.d1\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}d1.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} d1.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|l}
 missc\_error\\
\hline
	 0.2101449\\
\end{tabular}


    
    \paragraph{Missclasification error of maximal
tree}\label{missclasification-error-of-maximal-tree}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} max.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}fit.learn.max\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}max.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} max.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|l}
 missc\_error\\
\hline
	 0.08768116\\
\end{tabular}


    
    \paragraph{Missclasification error of best CV tree
model}\label{missclasification-error-of-best-cv-tree-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} cv.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}fit.learn.pruned.cv\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}cv.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} cv.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|l}
 missc\_error\\
\hline
	 0.07681159\\
\end{tabular}


    
    \paragraph{Missclasification error of 1SE tree
model}\label{missclasification-error-of-1se-tree-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} se.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}fit.learn.pruned.1se\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}se.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} se.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|l}
 missc\_error\\
\hline
	 0.07971014\\
\end{tabular}


    
    \subsection{3. Random Forests}\label{random-forests}

    \subsubsection{1. Load the library
randomForest}\label{load-the-library-randomforest}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k+kn}{library}\PY{p}{(}randomForest\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine


    \end{Verbatim}

    \subsubsection{2. Build a RF for mtry=p (unpruned bagging) and calculate
the gain in terms of error with respect to a single
tree}\label{build-a-rf-for-mtryp-unpruned-bagging-and-calculate-the-gain-in-terms-of-error-with-respect-to-a-single-tree}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{}?randomForest}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{p}{(}rf.mtry.p \PY{o}{\PYZlt{}\PYZhy{}} randomForest\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} learn\PY{p}{,} mtry \PY{o}{=} \PY{k+kp}{ncol}\PY{p}{(}learn\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}

Call:
 randomForest(formula = type ~ ., data = learn, mtry = ncol(learn) -      1) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 57

        OOB estimate of  error rate: 5.68%
Confusion matrix:
        nonspam spam class.error
nonspam    1870   82  0.04200820
spam        101 1168  0.07959023
    \end{verbatim}

    
    \paragraph{Missclasification error from the Random Forest
model}\label{missclasification-error-from-the-random-forest-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{} Predict}
         \PY{c+c1}{\PYZsh{}pred.rf.mtry.p \PYZlt{}\PYZhy{} predict(rf.mtry.p, test)}
         \PY{c+c1}{\PYZsh{}table(pred.rf.mtry.p, test\PYZdl{}type)}
         \PY{c+c1}{\PYZsh{} calculate accuracy pred.rf.mtry.p}
         \PY{c+c1}{\PYZsh{}sum(diag(table(pred.rf.mtry.p, test\PYZdl{}type))) / nrow(test) \PYZsh{} 0.95\PYZpc{} of accuracy}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} rf.mtry.p.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}rf.mtry.p\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 gain \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{==} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{,}
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}rf.mtry.p.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} rf.mtry.p.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}gain \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}gain\PY{p}{)}\PY{p}{,}
                    missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|ll}
 gain & missc\_error\\
\hline
	 0.9514493  & 0.04855072\\
\end{tabular}


    
    \subsubsection{3. Build a default RF}\label{build-a-default-rf}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{p}{(}rf.def \PY{o}{\PYZlt{}\PYZhy{}} randomForest\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} learn\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}

Call:
 randomForest(formula = type ~ ., data = learn) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 5.03%
Confusion matrix:
        nonspam spam class.error
nonspam    1892   60  0.03073770
spam        102 1167  0.08037825
    \end{verbatim}

    
    \subsubsection{4. Calculate an estimate of the prediction error and
compare it to
bagging}\label{calculate-an-estimate-of-the-prediction-error-and-compare-it-to-bagging}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} rf.def.error \PY{o}{\PYZlt{}\PYZhy{}} test \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          mutate\PY{p}{(}pred \PY{o}{=} predict\PY{p}{(}rf.def\PY{p}{,} test\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{class\PYZdq{}}\PY{p}{)}\PY{p}{,}    
                 gain \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{==} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{,}
                 error \PY{o}{=} \PY{k+kp}{ifelse}\PY{p}{(}pred \PY{o}{!=} type\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{p}{(}rf.def.missc\PYZus{}error \PY{o}{\PYZlt{}\PYZhy{}} rf.def.error \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}} 
          summarize\PY{p}{(}gain \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}gain\PY{p}{)}\PY{p}{,}
                    missc\PYZus{}error \PY{o}{=} \PY{k+kp}{mean}\PY{p}{(}error\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|ll}
 gain & missc\_error\\
\hline
	 0.957971   & 0.04202899\\
\end{tabular}


    
    \subsubsection{5. Study the evolution of the OOB error with respect to
ntree using
do.trace}\label{study-the-evolution-of-the-oob-error-with-respect-to-ntree-using-do.trace}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{c+c1}{\PYZsh{}rf.def.trace \PYZlt{}\PYZhy{} randomForest(type \PYZti{} ., data = learn, do.trace = TRUE)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} 
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        Error in xy.coords(x, y, xlabel, ylabel, log): 'x' and 'y' lengths differ
    Traceback:


        1. plot(x = rf.def.trace\$ntree, y = rf.def.trace\$oob.times)

        2. plot(x = rf.def.trace\$ntree, y = rf.def.trace\$oob.times)

        3. plot.default(x = rf.def.trace\$ntree, y = rf.def.trace\$oob.times)

        4. xy.coords(x, y, xlabel, ylabel, log)

        5. stop("'x' and 'y' lengths differ")

    \end{Verbatim}

    \subsection{4. Variable importance}\label{variable-importance}

    \subsubsection{1. Calculate the variable importance of the spam
variables for the default
RF}\label{calculate-the-variable-importance-of-the-spam-variables-for-the-default-rf}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{} ?importance}
         imp.def \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}importance\PY{p}{(}rf.def\PY{p}{)}\PY{p}{)}
         imp.def\PY{o}{\PYZdl{}}variable \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{rownames}\PY{p}{(}imp.def\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} See top 20 most important variables}
         arrange\PY{p}{(}imp.def\PY{p}{,} desc\PY{p}{(}MeanDecreaseGini\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{20}\PY{p}{,}\PY{p}{]}
\end{Verbatim}


    \begin{tabular}{r|ll}
 MeanDecreaseGini & variable\\
\hline
	 196.78870        & charExclamation \\
	 135.09368        & charDollar      \\
	 114.68567        & remove          \\
	  97.06415        & your            \\
	  95.90479        & capitalAve      \\
	  90.65938        & capitalLong     \\
	  89.62376        & free            \\
	  68.21056        & hp              \\
	  61.50052        & capitalTotal    \\
	  52.57840        & money           \\
	  49.19674        & our             \\
	  41.66793        & you             \\
	  34.24206        & num000          \\
	  31.61875        & george          \\
	  26.93517        & hpl             \\
	  21.58625        & business        \\
	  21.39635        & edu             \\
	  21.14260        & num1999         \\
	  18.46238        & charRoundbracket\\
	  17.83104        & will            \\
\end{tabular}


    
    \subsubsection{2. What are the most important
variables?}\label{what-are-the-most-important-variables}

    \textbf{Answer:} charExclamation, charDollar, remove

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{k+kn}{library}\PY{p}{(}ggplot2\PY{p}{)}
         
         \PY{k+kt}{data\PYZus{}frame}\PY{p}{(}variable \PY{o}{=} imp.def\PY{o}{\PYZdl{}}variable\PY{p}{,} importance \PY{o}{=} imp.def\PY{o}{\PYZdl{}}MeanDecreaseGini\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           mutate\PY{p}{(}importance \PY{o}{=} importance \PY{o}{/} \PY{k+kp}{sum}\PY{p}{(}importance\PY{p}{)}\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           top\PYZus{}n\PY{p}{(}\PY{l+m}{20}\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           ggplot\PY{p}{(}aes\PY{p}{(}x \PY{o}{=} importance\PY{p}{,}
                      y \PY{o}{=} reorder\PY{p}{(}variable\PY{p}{,} importance\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+}
           labs\PY{p}{(}title \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Importance of variables \PYZbs{}n in determining the type of email: \PYZbs{}n spam or non spam\PYZdq{}}\PY{p}{,}
                subtitle \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Twenty most relevant variables for default Random Forest model \PYZbs{}n (variables scaled to sum 1)\PYZdq{}}\PY{p}{)} \PY{o}{+}
           theme\PYZus{}bw\PY{p}{(}\PY{p}{)} \PY{o}{+}
           theme\PY{p}{(}axis.title.y \PY{o}{=} element\PYZus{}blank\PY{p}{(}\PY{p}{)}\PY{p}{,}
                 plot.title \PY{o}{=} element\PYZus{}text\PY{p}{(}hjust \PY{o}{=} \PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                 plot.subtitle \PY{o}{=} element\PYZus{}text\PY{p}{(}hjust \PY{o}{=} \PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                 axis.line \PY{o}{=} element\PYZus{}line\PY{p}{(}colour \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{grey\PYZdq{}}\PY{p}{)}\PY{p}{,}
                 panel.grid.major \PY{o}{=} element\PYZus{}blank\PY{p}{(}\PY{p}{)}\PY{p}{,} panel.border \PY{o}{=} element\PYZus{}blank\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+}
           geom\PYZus{}segment\PY{p}{(}aes\PY{p}{(}x \PY{o}{=} \PY{o}{\PYZhy{}}\PY{k+kc}{Inf}\PY{p}{,} y \PY{o}{=} reorder\PY{p}{(}variable\PY{p}{,} importance\PY{p}{)}\PY{p}{,}
                            xend \PY{o}{=} importance\PY{p}{,} yend \PY{o}{=} reorder\PY{p}{(}variable\PY{p}{,} importance\PY{p}{)}\PY{p}{)}\PY{p}{,}
                        size \PY{o}{=} \PY{l+m}{0.2}\PY{p}{)} \PY{o}{+} 
           geom\PYZus{}point\PY{p}{(}color \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{orange\PYZdq{}}\PY{p}{,} cex \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Selecting by importance

    \end{Verbatim}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{3. Calculate the importance of spam variables for stumps
RF}\label{calculate-the-importance-of-spam-variables-for-stumps-rf}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{p}{(}rf.d1 \PY{o}{\PYZlt{}\PYZhy{}} randomForest\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} learn\PY{p}{,} maxnodes \PY{o}{=} \PY{l+m}{2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}

Call:
 randomForest(formula = type ~ ., data = learn, maxnodes = 2) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 17.17%
Confusion matrix:
        nonspam spam class.error
nonspam    1932   20   0.0102459
spam        533  736   0.4200158
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{c+c1}{\PYZsh{} ?importance}
         imp.d1 \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}importance\PY{p}{(}rf.d1\PY{p}{)}\PY{p}{)}
         imp.d1\PY{o}{\PYZdl{}}variable \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{rownames}\PY{p}{(}imp.d1\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} See top 20 most important variables}
         arrange\PY{p}{(}imp.d1\PY{p}{,} desc\PY{p}{(}MeanDecreaseGini\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{20}\PY{p}{,}\PY{p}{]}
\end{Verbatim}


    \begin{tabular}{r|ll}
 MeanDecreaseGini & variable\\
\hline
	 64.301241       & charDollar     \\
	 42.598855       & charExclamation\\
	 39.150650       & remove         \\
	 32.699291       & free           \\
	 27.944416       & capitalAve     \\
	 27.806511       & your           \\
	 19.930254       & money          \\
	 19.842940       & capitalLong    \\
	 11.904380       & num000         \\
	 11.412404       & our            \\
	 11.179651       & capitalTotal   \\
	  5.304552       & hp             \\
	  3.962731       & internet       \\
	  3.607299       & business       \\
	  3.323022       & hpl            \\
	  2.992415       & all            \\
	  2.913039       & you            \\
	  2.740771       & receive        \\
	  2.718170       & george         \\
	  1.551939       & credit         \\
\end{tabular}


    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{k+kt}{data\PYZus{}frame}\PY{p}{(}variable \PY{o}{=} imp.d1\PY{o}{\PYZdl{}}variable\PY{p}{,} importance \PY{o}{=} imp.d1\PY{o}{\PYZdl{}}MeanDecreaseGini\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           mutate\PY{p}{(}importance \PY{o}{=} importance \PY{o}{/} \PY{k+kp}{sum}\PY{p}{(}importance\PY{p}{)}\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           top\PYZus{}n\PY{p}{(}\PY{l+m}{20}\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           ggplot\PY{p}{(}aes\PY{p}{(}x \PY{o}{=} importance\PY{p}{,}
                      y \PY{o}{=} reorder\PY{p}{(}variable\PY{p}{,} importance\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+}
           labs\PY{p}{(}title \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Importance of variables \PYZbs{}n in determining the type of email: \PYZbs{}n spam or non spam\PYZdq{}}\PY{p}{,}
                subtitle \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Twenty most relevant variables for stump Random Forest model \PYZbs{}n (variables scaled to sum 1)\PYZdq{}}\PY{p}{)} \PY{o}{+}
           theme\PYZus{}bw\PY{p}{(}\PY{p}{)} \PY{o}{+}
           theme\PY{p}{(}axis.title.y \PY{o}{=} element\PYZus{}blank\PY{p}{(}\PY{p}{)}\PY{p}{,}
                 plot.title \PY{o}{=} element\PYZus{}text\PY{p}{(}hjust \PY{o}{=} \PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                 plot.subtitle \PY{o}{=} element\PYZus{}text\PY{p}{(}hjust \PY{o}{=} \PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                 axis.line \PY{o}{=} element\PYZus{}line\PY{p}{(}colour \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{grey\PYZdq{}}\PY{p}{)}\PY{p}{,}
                 panel.grid.major \PY{o}{=} element\PYZus{}blank\PY{p}{(}\PY{p}{)}\PY{p}{,} panel.border \PY{o}{=} element\PYZus{}blank\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+}
           geom\PYZus{}segment\PY{p}{(}aes\PY{p}{(}x \PY{o}{=} \PY{o}{\PYZhy{}}\PY{k+kc}{Inf}\PY{p}{,} y \PY{o}{=} reorder\PY{p}{(}variable\PY{p}{,} importance\PY{p}{)}\PY{p}{,}
                            xend \PY{o}{=} importance\PY{p}{,} yend \PY{o}{=} reorder\PY{p}{(}variable\PY{p}{,} importance\PY{p}{)}\PY{p}{)}\PY{p}{,}
                        size \PY{o}{=} \PY{l+m}{0.2}\PY{p}{)} \PY{o}{+} 
           geom\PYZus{}point\PY{p}{(}color \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{orange\PYZdq{}}\PY{p}{,} cex \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Selecting by importance

    \end{Verbatim}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_70_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{4. Illustrate the influence of the mtry parameter on the
OOB error and on the
VI}\label{illustrate-the-influence-of-the-mtry-parameter-on-the-oob-error-and-on-the-vi}

    \subsection{5. Variable selection using random
forests}\label{variable-selection-using-random-forests}

    \subsubsection{\texorpdfstring{1. Load the library
\textbf{VSURF}}{1. Load the library VSURF}}\label{load-the-library-vsurf}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k+kn}{library}\PY{p}{(}VSURF\PY{p}{)}
\end{Verbatim}


    \subsubsection{\texorpdfstring{2. Apply \textbf{VSURF} on a subset of
500 observations of the data table
spam.app}{2. Apply VSURF on a subset of 500 observations of the data table spam.app}}\label{apply-vsurf-on-a-subset-of-500-observations-of-the-data-table-spam.app}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{} Subset spam data using stratified sampling}
         spam.app \PY{o}{\PYZlt{}\PYZhy{}} 
           spam \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           group\PYZus{}by\PY{p}{(}type\PY{p}{)} \PY{o}{\PYZpc{}\PYZgt{}\PYZpc{}}
           sample\PYZus{}n\PY{p}{(}\PY{l+m}{250}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{c+c1}{\PYZsh{} Apply VSURF}
         vsurf.spam \PY{o}{\PYZlt{}\PYZhy{}} VSURF\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} spam.app\PY{p}{,} parallel \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} clusterType \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{FORK\PYZdq{}}\PY{p}{,} ncores \PY{o}{=} detectCores\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, parallel = TRUE, clusterType = "FORK", :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} plot\PY{p}{(}vsurf.spam\PY{p}{,} cex.axis \PY{o}{=} \PY{l+m}{1.1}\PY{p}{,} cex.lab \PY{o}{=} \PY{l+m}{1.2}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{3. Comment on the results of the different
steps}\label{comment-on-the-results-of-the-different-steps}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{k+kp}{summary}\PY{p}{(}vsurf.spam\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

 VSURF computation time: 1.3 mins 

 VSURF selected: 
	51 variables at thresholding step (in 17.8 secs)
	1 variables at interpretation step (in 57.3 secs)
	1 variables at prediction step (in 0.8 secs)

 VSURF ran in parallel on a FORK cluster and used 7 cores 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{} Thresholding variables}
         \PY{k+kp}{colnames}\PY{p}{(}spam.app\PY{p}{[}vsurf.spam\PY{o}{\PYZdl{}}varselect.thres\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{enumerate*}
\item 'type'
\item 'hp'
\item 'money'
\item 'george'
\item 'remove'
\item 'charDollar'
\item 'charExclamation'
\item 'free'
\item 'capitalAve'
\item 'capitalTotal'
\item 'edu'
\item 'capitalLong'
\item 'hpl'
\item 'num000'
\item 'your'
\item 'num1999'
\item 'our'
\item 'you'
\item 'business'
\item 'over'
\item 'num85'
\item 'internet'
\item 'will'
\item 'meeting'
\item 'email'
\item 'num650'
\item 'order'
\item 'credit'
\item 'lab'
\item 'receive'
\item 're'
\item 'telnet'
\item 'labs'
\item 'charRoundbracket'
\item 'data'
\item 'pm'
\item 'charSemicolon'
\item 'addresses'
\item 'mail'
\item 'charSquarebracket'
\item 'project'
\item 'charHash'
\item 'technology'
\item 'original'
\item 'address'
\item 'all'
\item 'num415'
\item 'num857'
\item 'cs'
\item 'font'
\item 'people'
\end{enumerate*}


    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{c+c1}{\PYZsh{} Interpretation variables}
         \PY{k+kp}{colnames}\PY{p}{(}spam.app\PY{p}{[}vsurf.spam\PY{o}{\PYZdl{}}varselect.interp\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    'type'

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{} Prediction variables}
         \PY{k+kp}{colnames}\PY{p}{(}spam.app\PY{p}{[}vsurf.spam\PY{o}{\PYZdl{}}varselect.pred\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    'type'

    
    \paragraph{Stump RF model}\label{stump-rf-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} vsurf.stump \PY{o}{\PYZlt{}\PYZhy{}} VSURF\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} spam.app\PY{p}{,} maxnodes \PY{o}{=} \PY{l+m}{2}\PY{p}{,} parallel \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} clusterType \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{FORK\PYZdq{}}\PY{p}{,} ncores \PY{o}{=} detectCores\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Warning message in VSURF.formula(type \textasciitilde{} ., spam.app, maxnodes = 2, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{k+kp}{summary}\PY{p}{(}vsurf.stump\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

 VSURF computation time: 40.6 secs 

 VSURF selected: 
	58 variables at thresholding step (in 8.5 secs)
	1 variables at interpretation step (in 31.6 secs)
	1 variables at prediction step (in 0.5 secs)

 VSURF ran in parallel on a FORK cluster and used 7 cores 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} plot\PY{p}{(}vsurf.stump\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_87_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{\texorpdfstring{4. Experiment with the parallel version
based on the article on
\textbf{VSURF}}{4. Experiment with the parallel version based on the article on VSURF}}\label{experiment-with-the-parallel-version-based-on-the-article-on-vsurf}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{c+c1}{\PYZsh{} Try different thresholds values (tuning the function)}
          
          \PY{c+c1}{\PYZsh{} Number of forest to grown values for each step (from 10 to 100)}
          nfor.thres.values \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{seq}\PY{p}{(}\PY{l+m}{0}\PY{p}{,} \PY{l+m}{100}\PY{p}{,} by \PY{o}{=} \PY{l+m}{10}\PY{p}{)}\PY{p}{[}\PY{l+m}{\PYZhy{}1}\PY{p}{]}
          nfor.interp.values \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{seq}\PY{p}{(}\PY{l+m}{0}\PY{p}{,} \PY{l+m}{100}\PY{p}{,} by \PY{o}{=} \PY{l+m}{10}\PY{p}{)}\PY{p}{[}\PY{l+m}{\PYZhy{}1}\PY{p}{]}
          nfor.pred.values \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{seq}\PY{p}{(}\PY{l+m}{0}\PY{p}{,} \PY{l+m}{100}\PY{p}{,} by \PY{o}{=} \PY{l+m}{10}\PY{p}{)}\PY{p}{[}\PY{l+m}{\PYZhy{}1}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} All combinations for the different nfor values in each step}
          
          list.nfor.thres \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{lapply}\PY{p}{(}nfor.thres.values\PY{p}{,} \PY{k+kr}{function}\PY{p}{(}x\PY{p}{)} \PY{p}{\PYZob{}}
              \PY{k+kp}{message}\PY{p}{(}\PY{k+kp}{paste0}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZbs{}n Calculating VSURF with nfor.thres = \PYZdq{}}\PY{p}{,} x\PY{p}{)}\PY{p}{)}
              VSURF\PY{p}{(}type \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,} data \PY{o}{=} spam.app\PY{p}{,} nfor.thres \PY{o}{=} x\PY{p}{,} parallel \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} clusterType \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{PSOCK\PYZdq{}}\PY{p}{,} ncores \PY{o}{=} detectCores\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m}{1}\PY{p}{)}
          \PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Calculating VSURF with nfor.thres = 10
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 20
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 30
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 40
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 50
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 60
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 70
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 80
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 90
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”Calculating VSURF with nfor.thres = 100
Warning message in VSURF.formula(type \textasciitilde{} ., data = spam.app, nfor.thres = x, parallel = TRUE, :
“VSURF with a formula-type call outputs selected variables
which are indices of the input matrix based on the formula:
you may reorder these to get indices of the original data”
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} number.threshold.vars \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{lapply}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{9}\PY{p}{,} \PY{k+kr}{function}\PY{p}{(}x\PY{p}{)} \PY{p}{\PYZob{}}
              \PY{k+kp}{message}\PY{p}{(}\PY{k+kp}{paste0}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZbs{}n Getting number of threshold variables for \PYZdq{}}\PY{p}{)}\PY{p}{,} nfor.thres.values\PY{p}{[}x\PY{p}{]}\PY{p}{)}
              \PY{k+kp}{length}\PY{p}{(}list.nfor.thres\PY{p}{[[}x\PY{p}{]]}\PY{o}{\PYZdl{}}varselect.thres\PY{p}{)}
          \PY{p}{\PYZcb{}}\PY{p}{)}
          
          number.threshold.vars
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

 Getting number of threshold variables for 10

 Getting number of threshold variables for 20

 Getting number of threshold variables for 30

 Getting number of threshold variables for 40

 Getting number of threshold variables for 50

 Getting number of threshold variables for 60

 Getting number of threshold variables for 70

 Getting number of threshold variables for 80

 Getting number of threshold variables for 90

    \end{Verbatim}

    \begin{enumerate}
\item 49
\item 51
\item 51
\item 54
\item 54
\item 56
\item 53
\item 54
\item 53
\end{enumerate}


    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}139}]:} \PY{c+c1}{\PYZsh{} Make a plot with number of thresholds variables vs number of forest threshold (nfor.thres)}
          ggplot\PY{p}{(}\PY{p}{)}  \PY{o}{+} 
            geom\PYZus{}line\PY{p}{(}aes\PY{p}{(}x \PY{o}{=} nfor.thres.values\PY{p}{[}\PY{l+m}{\PYZhy{}10}\PY{p}{]}\PY{p}{,} y \PY{o}{=} \PY{k+kp}{unlist}\PY{p}{(}number.threshold.vars\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+} 
            geom\PYZus{}point\PY{p}{(}aes\PY{p}{(}x \PY{o}{=} nfor.thres.values\PY{p}{[}\PY{l+m}{\PYZhy{}10}\PY{p}{]}\PY{p}{,} y \PY{o}{=} \PY{k+kp}{unlist}\PY{p}{(}number.threshold.vars\PY{p}{)}\PY{p}{)}\PY{p}{,} color \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{orange\PYZdq{}}\PY{p}{,} cex \PY{o}{=} \PY{l+m}{2}\PY{p}{)} \PY{o}{+} 
            labs\PY{p}{(}x \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{nfor.thres values\PYZdq{}}\PY{p}{,} y \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Number of threshold variables obtained\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{c+c1}{\PYZsh{} Para cada combinación hacer un plot que muestre el número de threshold variables, interpretation variables and predicted variables}
          \PY{c+c1}{\PYZsh{} Buscar las variables que se repiten}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        Error in parse(text = x, srcfile = src): <text>:8:0: unexpected end of input
    6: \# Para cada combinación hacer un plot que muestre el número de threshold variables, interpretation variables and predicted variables
    7: \# Buscar las variables que se repiten 
      \^{}
    Traceback:


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{o}{?}VSURF
\end{Verbatim}


    \inputencoding{utf8}
\HeaderA{VSURF}{Variable Selection Using Random Forests}{VSURF}
\methaliasA{VSURF.default}{VSURF}{VSURF.default}
\methaliasA{VSURF.formula}{VSURF}{VSURF.formula}
%
\begin{Description}\relax
Three steps variable selection procedure based on random forests for
supervised classification and regression problems.  First step
("thresholding step") is dedicated to eliminate irrelevant variables from
the dataset.  Second step ("interpretation step") aims to select all
variables related to the response for interpretation prupose.  Third step
("prediction step") refines the selection by eliminating redundancy in the
set of variables selected by the second step, for prediction prupose.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
VSURF(x, ...)

## Default S3 method:
VSURF(x, y, ntree = 2000, mtry = max(floor(ncol(x)/3), 1),
  nfor.thres = 50, nmin = 1, nfor.interp = 25, nsd = 1,
  nfor.pred = 25, nmj = 1, parallel = FALSE, ncores = detectCores() - 1,
  clusterType = "PSOCK", ...)

## S3 method for class 'formula'
VSURF(formula, data, ..., na.action = na.fail)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, formula}] A data frame or a matrix of predictors, the columns
represent the variables. Or a formula describing the model to be fitted.

\item[\code{...}] others parameters to be passed on to the \code{randomForest}
function (see ?randomForest for further information).

\item[\code{y}] A response vector (must be a factor for classification problems and
numeric for regression ones).

\item[\code{ntree}] Number of trees in each forests grown. Standard parameter of
\code{randomForest}.

\item[\code{mtry}] Number of variables randomly sampled as candidates at each
split.  Standard parameter of \code{randomForest}.

\item[\code{nfor.thres}] Number of forests grown for "thresholding step" (first of
the three steps).

\item[\code{nmin}] Number of times the "minimum value" is multiplied to set
threshold value.

\item[\code{nfor.interp}] Number of forests grown for "intepretation step" (second
of the three steps).

\item[\code{nsd}] Number of times the standard deviation of the minimum value of
\code{err.interp} is multiplied.

\item[\code{nfor.pred}] Number of forests grown for "prediction step" (last of the
three steps).

\item[\code{nmj}] Number of times the mean jump is multiplied.

\item[\code{parallel}] A logical indicating if you want VSURF to run in parallel on
multiple cores (default to FALSE).

\item[\code{ncores}] Number of cores to use. Default is set to the number of cores
detected by R minus 1.

\item[\code{clusterType}] Type of the multiple cores cluster used to run VSURF in
parallel. Must be chosen among "PSOCK" (default: SOCKET cluster available
locally on all OS), "FORK" (local too, only available for Linux and Mac OS)
and "MPI" (can be used on a remote cluster, which needs \code{snow} and
\code{Rmpi} packages installed).

\item[\code{data}] a data frame containing the variables in the model.

\item[\code{na.action}] A function to specify the action to be taken if NAs are
found.  (NOTE: If given, this argument must be named, and as
\code{randomForest} it is only used with the formula-type call.)
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\begin{itemize}
 \item First step ("thresholding step"): first, \code{nfor.thres}
random forests are computed using the function \code{randomForest} with
arguments \code{importance=TRUE}, and our choice of default values for 
\code{ntree} and \code{mtry} (which are higher than default in
\code{\LinkA{randomForest}{randomForest}} to get a more stable variable importance measure).
Then variables are sorted according to their mean variable importance (VI),
in decreasing order.  This order is kept all along the procedure.
Next, a threshold is computed:
\code{min.thres}, the minimum predicted value of a pruned CART tree fitted
to the curve of the standard deviations of VI.  Finally, the actual
"thresholding step" is performed: only variables with a mean VI larger than
\code{nmin} * \code{min.thres} are kept.

\item Second step ("intepretation step"): the variables selected by the
first step are considered. \code{nfor.interp} embedded random forests models
are grown, starting with the random forest build with only the most
important variable and ending with all variables selected in the first step.
Then, \code{err.min} the minimum mean out-of-bag (OOB) error of these models
and its associated standard deviation \code{sd.min} are computed.  Finally,
the smallest model (and hence its corresponding variables) having a mean OOB
error less than \code{err.min} + \code{nsd} * \code{sd.min} is selected.

Note that for this step (and the next one),
the \code{mtry} parameter of \code{randomForest} is set to its default value
(see \code{\LinkA{randomForest}{randomForest}}) if \code{nvm}, the number of variables
in the model, is not greater than the number of observations,
while it is set to \code{nvm/3} otherwise. This is to ensure quality of OOB
error estimations along embedded RF models.

\item Third step ("prediction step"): the starting point is the same than in
the second step. However, now the variables are added to the model in a
stepwise manner. \code{mean.jump}, the mean jump value is calculated using
variables that have been left out by the second step, and is set as the mean
absolute difference between mean OOB errors of one model and its first
following model.  Hence a variable is included in the model if the mean OOB
error decrease is larger than \code{nmj} * \code{mean.jump}.

As for interpretation step,
the \code{mtry} parameter of \code{randomForest} is set to its default value
if \code{nvm}, the number of variables
in the model, is not greater than the number of observations,
while it is set to \code{nvm/3} otherwise.
\end{itemize}


VSURF is able to run using mutliple cores in parallel
(see \code{parallel}, \code{clusterType} and \code{ncores} arguments).
\end{Details}
%
\begin{Value}
An object of class \code{VSURF}, which is a list with the following
components:

\begin{ldescription}
\item[\code{varselect.thres}] A vector of indexes of variables selected after
"thresholding step", sorted according to their mean VI, in decreasing order.

\item[\code{varselect.interp}] A vector of indexes of variables selected after
"interpretation step".

\item[\code{varselect.pred}] A vector of indexes of variables selected after
"prediction step".

\item[\code{nums.varselect}] A vector of the 3 numbers of variables selected
resp. by "thresholding step", "interpretation step" and "prediction step".

\item[\code{imp.varselect.thres}] A vector of importances of the
\code{varselect.thres} variables.

\item[\code{min.thres}] The minimum predicted value of a pruned CART tree
fitted to the curve of the standard deviations of VI.

\item[\code{imp.mean.dec}] A vector of the variables importance means
(over \code{nfor.thres} runs), in decreasing order.

\item[\code{imp.mean.dec.ind}] The ordering index vector associated to the sorting
of variables importance means.

\item[\code{imp.sd.dec}] A vector of standard deviations of all variables
importances. The order is given by \code{imp.mean.dec.ind}.

\item[\code{mean.perf}] Mean OOB error rate, obtained by a random forests
build on all variables.

\item[\code{pred.pruned.tree}] Predictions of the CART tree fitted to the
curve of the standard deviations of VI.

\item[\code{err.interp}] A vector of the mean OOB error rates of the embedded
random forests models build during the "interpretation step".

\item[\code{sd.min}] The standard deviation of OOB error rates associated to
the random forests model attaining the minimum mean OOB error rate during
the "interpretation step".

\item[\code{err.pred}] A vector of the mean OOB error rates of the random
forests models build during the "prediction step".

\item[\code{mean.jump}] The mean jump value computed during the "prediction
step".

\item[\code{nmin,nsd,nmj}] Corresponding parameters values.

\item[\code{overall.time}] Overall computation time.

\item[\code{comput.times}] A list of the 3 computation times respectively
associated with the 3 steps: "thresholding", "interpretation" and
"prediction".

\item[\code{ncores}] The number of cores used to run \code{VSURF}
in parallel (NULL if VSURF did not run in parallel).

\item[\code{clusterType}] The type of the cluster used to run
\code{VSURF} in parallel (NULL if VSURF did not run in parallel).

\item[\code{call}] The original call to \code{VSURF}.

\item[\code{terms}] Terms associated to the formula (only if formula-type call
was used).

\item[\code{na.action}] Method used to deal with missing values (only if formula-type call
was used).
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Robin Genuer, Jean-Michel Poggi and Christine Tuleau-Malot
\end{Author}
%
\begin{References}\relax
Genuer, R. and Poggi, J.M. and Tuleau-Malot, C. (2010),
\emph{Variable selection using random forests}, Pattern Recognition Letters
31(14), 2225-2236

Genuer, R. and Poggi, J.M. and Tuleau-Malot, C. (2015),
\emph{VSURF: An R Package for Variable Selection Using Random Forests},
The R Journal 7(2):19-33
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.VSURF}{plot.VSURF}}, \code{\LinkA{summary.VSURF}{summary.VSURF}},
\code{\LinkA{VSURF\_thres}{VSURF.Rul.thres}}, \code{\LinkA{VSURF\_interp}{VSURF.Rul.interp}},
\code{\LinkA{VSURF\_pred}{VSURF.Rul.pred}}, \code{\LinkA{tune}{tune}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

data(iris)
iris.vsurf <- VSURF(iris[,1:4], iris[,5], ntree = 100, nfor.thres = 20,
                    nfor.interp = 10, nfor.pred = 10)
iris.vsurf

## Not run: 
# A more interesting example with toys data (see \code{\link{toys}})
# (a few minutes to execute)
data(toys)
toys.vsurf <- VSURF(toys$x, toys$y)
toys.vsurf

# VSURF run on 2 cores in parallel (using a SOCKET cluster):
data(toys)
toys.vsurf.parallel <- VSURF(toys$x, toys$y, parallel = TRUE, ncores = 2)

## End(Not run)

\end{ExampleCode}
\end{Examples}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Make a combination of diferent thresholds and create a raster view}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
